{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "#from tqdm import trange,tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext import data\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs=2\n",
    "num_classes = 5\n",
    "batch_size=32\n",
    "data_path='./data/'\n",
    "vectors = Vectors('glove.twitter.27B.200d.txt', 'C:/Users/YYH/Desktop/nlp-beginner/Task 2/embedding/')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器\n",
    "\n",
    "def load_iters(batch_size=32, device=\"cpu\", data_path='data', vectors=None):\n",
    "    # 加载Field 已包含数据预处理\n",
    "    TEXT = data.Field(lower=True, batch_first=True, include_lengths=True)\n",
    "    LABEL = data.LabelField(batch_first=True)\n",
    "    # 标明数据文件中的栏位\n",
    "    train_fields = [(None, None), (None, None), ('text', TEXT), ('label', LABEL)]\n",
    "    test_fields = [(None, None), (None, None), ('text', TEXT)]\n",
    "    train_data = data.TabularDataset.splits(\n",
    "        path=data_path,\n",
    "        train='train.tsv',\n",
    "        format='tsv',\n",
    "        fields=train_fields,\n",
    "        skip_header=True\n",
    "    )[0]\n",
    "\n",
    "    test_data = data.TabularDataset.splits(\n",
    "        path='data',\n",
    "        train='test.tsv',\n",
    "        format='tsv',\n",
    "        fields=test_fields,\n",
    "        skip_header=True\n",
    "    )[0]\n",
    "    TEXT.build_vocab(train_data.text, vectors=vectors)\n",
    "    LABEL.build_vocab(train_data.label)\n",
    "    train_data, dev_data = train_data.split([0.8, 0.2])\n",
    "\n",
    "    train_iter, dev_iter = BucketIterator.splits(\n",
    "        (train_data, dev_data),\n",
    "        batch_sizes=(batch_size, batch_size),\n",
    "        device=device,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        repeat=False,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_iter = Iterator(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        sort=False,\n",
    "        sort_within_batch=False,\n",
    "        repeat=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_iter, dev_iter, test_iter, TEXT, LABEL\n",
    "train_iter, dev_iter, test_iter, TEXT, LABEL = load_iters(batch_size, device, data_path, vectors)\n",
    "vocab_size = len(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YYH\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:57: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# 定义LSTM模型\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, output_dim,  \n",
    "                 bidirectional, dropout):\n",
    "        super().__init__()          \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        dense_outputs=self.fc(hidden)\n",
    "        outputs=self.act(dense_outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "embed_size = 200\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.1\n",
    "\n",
    "lstm_model = LSTM(vocab_size, embed_size, hidden_size, num_layers, num_classes, bidirectional, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义textCNN模型\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim, \n",
    "        kernel_sizes, \n",
    "        num_filters, \n",
    "        num_classes, dropout_rate):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embed_size), padding=(k - 1, 0))\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x).squeeze(3))\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x,lens):\n",
    "        embed = self.embedding(x).unsqueeze(1)\n",
    "        conv_results = [self.conv_and_pool(embed, conv) for conv in self.convs]\n",
    "        out = torch.cat(conv_results, 1)\n",
    "        return self.fc(self.dropout(out))\n",
    "\n",
    "embed_size = 200\n",
    "kernel_sizes=[3, 4, 5]\n",
    "num_filters=100\n",
    "dropout_rate = 0.1\n",
    "\n",
    "cnn_model=TextCNN(vocab_size,embed_size,kernel_sizes,num_filters,num_classes,dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model, loss_fn, optimizer, train_generator, dev_generator,epochs):\n",
    "    model.to(device)\n",
    "    loss_history=[]\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(tqdm(train_generator)):\n",
    "            model.train()\n",
    "            (inputs, lens), labels = batch.text, batch.label\n",
    "            if 0 in lens:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            forward_output = model(inputs,lens)\n",
    "            loss = loss_fn(forward_output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 10 == 0:\n",
    "                loss_history.append(loss.item())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            corr_num = 0\n",
    "            err_num = 0\n",
    "            for batch in dev_iter:\n",
    "                (inputs, lens), labels = batch.text, batch.label\n",
    "                if 0 in lens:\n",
    "                    continue\n",
    "                outputs = model(inputs, lens)\n",
    "                corr_num += (outputs.argmax(1) == labels).sum().item()\n",
    "                err_num += (outputs.argmax(1) != labels).sum().item()\n",
    "            tqdm.write('Epoch {}, Accuracy {}'.format(epoch, corr_num / (corr_num + err_num))) \n",
    "        torch.save(model, './model/model_'+model_name+'_epoch_{}.pkl'.format(epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f92003d5f746c8a198c4c1adbd47e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy 0.646802511854415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816554799ce14c5eb34ac4b924e7ebfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy 0.6678200692041523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(16533, 200)\n",
       "  (lstm): LSTM(200, 256, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (act): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=lstm_model\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train(model,loss_fn,optimizer,train_iter,dev_iter,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca9ae32bea24e5fb96051d96771bfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy 0.6266500064077919\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65457419e3bc4c95bee24dbb84fb890a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy 0.6491093169293861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(16533, 200)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 200), stride=(1, 1), padding=(2, 0))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 200), stride=(1, 1), padding=(3, 0))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 200), stride=(1, 1), padding=(4, 0))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=cnn_model\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train(model,loss_fn,optimizer,train_iter,dev_iter,epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
