{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext import data\n",
    "import torch\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 36\n",
    "hidden_size = 600  # every LSTM's(forward and backward) hidden size is half of HIDDEN_SIZE\n",
    "epochs = 1\n",
    "dropout = 0.2\n",
    "num_layers = 2\n",
    "num_classes=4\n",
    "learning_rate = 4e-4\n",
    "embedding_size = 200\n",
    "\n",
    "data_path='./data/'\n",
    "vectors = Vectors('glove.twitter.27B.200d.txt', 'C:/Users/YYH/Desktop/nlp-beginner/Task 2/embedding/')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iter(batch_size=32, device=\"cuda\", data_path='./data/', vectors=None):\n",
    "    TEXT = data.Field(batch_first=True, include_lengths=True, lower=True)\n",
    "    LABEL = data.LabelField(batch_first=True)\n",
    "    fields = {'sentence1': ('premise', TEXT),\n",
    "              'sentence2': ('hypothesis', TEXT),\n",
    "              'gold_label': ('label', LABEL)}\n",
    "    train_data, dev_data, test_data = data.TabularDataset.splits(\n",
    "        path=data_path,\n",
    "        train='snli_1.0_train.jsonl',\n",
    "        validation='snli_1.0_dev.jsonl',\n",
    "        test='snli_1.0_test.jsonl',\n",
    "        format='json',\n",
    "        fields=fields,\n",
    "        filter_pred=lambda ex: ex.label != '-'\n",
    "    )\n",
    "    TEXT.build_vocab(train_data, vectors=vectors, unk_init=torch.Tensor.normal_)\n",
    "    LABEL.build_vocab(test_data)\n",
    "    train_iter, dev_iter = BucketIterator.splits(\n",
    "        (train_data, dev_data),\n",
    "        batch_sizes=(batch_size, batch_size),\n",
    "        device=device,\n",
    "        sort_key=lambda x: len(x.premise) + len(x.hypothesis),\n",
    "        sort_within_batch=True,\n",
    "        repeat=False,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_iter = Iterator(\n",
    "         test_data,\n",
    "         batch_size=batch_size,\n",
    "         device=device,\n",
    "         sort=False,\n",
    "         sort_within_batch=False,\n",
    "         repeat=False,\n",
    "         shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_iter, dev_iter, test_iter, TEXT, LABEL\n",
    "\n",
    "train_iter, dev_iter, test_iter, TEXT, LABEL = load_iter(batch_size, device, data_path, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESIM(\n",
       "  (Input_Encoding): Input_Encoding(\n",
       "    (embedding): Embedding(56220, 200)\n",
       "    (lstm): LSTM(200, 600, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (Local_Inference): Local_Inference()\n",
       "  (Inference_Composition): Inference_Composition(\n",
       "    (fc): Linear(in_features=4800, out_features=600, bias=True)\n",
       "    (lstm): LSTM(600, 600, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (Prediction_Layer): Prediction_Layer(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=4800, out_features=600, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=600, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Input_Encoding(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, bidirectional, dropout):\n",
    "        super(Input_Encoding,self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_size, \n",
    "                           hidden_size, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "                                                            text_lengths,\n",
    "                                                            batch_first=True,\n",
    "                                                            enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output\n",
    "\n",
    "class Local_Inference(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Local_Inference,self).__init__()\n",
    "        \n",
    "    def forward(self,a,b):\n",
    "        e=torch.matmul(a,b.transpose(1,2))\n",
    "        a1=nn.Softmax(dim=2)(e).bmm(b)\n",
    "        b1=nn.Softmax(dim=1)(e).transpose(1,2).bmm(a)\n",
    "        m_a=torch.cat([a,a1,a-a1,a*a1],dim=-1)\n",
    "        m_b=torch.cat([b,b1,b-b1,b*b1],dim=-1)\n",
    "        return m_a,m_b\n",
    "\n",
    "class Inference_Composition(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, bidirectional, dropout):\n",
    "        super(Inference_Composition, self).__init__()\n",
    "        self.fc=nn.Linear(8*hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, \n",
    "                           hidden_size, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,text,text_lengths):\n",
    "        x = self.fc(text)\n",
    "        x = self.dropout(x)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(x, \n",
    "                                                            text_lengths,\n",
    "                                                            batch_first=True,\n",
    "                                                            enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output\n",
    "\n",
    "class Prediction_Layer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, dropout):\n",
    "        super(Prediction_Layer, self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(8*hidden_size,hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size,num_classes)\n",
    "            )\n",
    "    def forward(self,a,b):\n",
    "        a1 = F.avg_pool1d(a.transpose(1, 2), a.size(1)).squeeze(-1)\n",
    "        a2 = F.max_pool1d(a.transpose(1, 2), a.size(1)).squeeze(-1)\n",
    "        b1 = F.avg_pool1d(b.transpose(1, 2), b.size(1)).squeeze(-1)\n",
    "        b2 = F.max_pool1d(b.transpose(1, 2), b.size(1)).squeeze(-1)\n",
    "        output = torch.cat((a1,a2,b1,b2), dim=-1)\n",
    "        output= self.fc(output)\n",
    "        return output\n",
    "        \n",
    "class ESIM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_classes=4, num_layers=2, \n",
    "                 dropout=0.2, bidirectional=True, batch_first=True, freeze=False):\n",
    "        super(ESIM, self).__init__()\n",
    "        self.name='ESIM'\n",
    "        self.Input_Encoding=Input_Encoding(embedding_size, hidden_size, num_layers, bidirectional, dropout)\n",
    "        self.Local_Inference=Local_Inference()\n",
    "        self.Inference_Composition=Inference_Composition(hidden_size, num_layers, bidirectional, dropout)\n",
    "        self.Prediction_Layer=Prediction_Layer(hidden_size, num_classes, dropout)\n",
    "        \n",
    "    def forward(self, a, a_length, b, b_length):\n",
    "        a0=self.Input_Encoding(a,a_length)\n",
    "        b0=self.Input_Encoding(b,b_length)\n",
    "        a1,b1=self.Local_Inference(a0,b0)\n",
    "        a2=self.Inference_Composition(a1,a_length)\n",
    "        b2=self.Inference_Composition(b1,b_length)\n",
    "        output=self.Prediction_Layer(a2,b2)\n",
    "        return output\n",
    "\n",
    "model=ESIM(embedding_size, hidden_size, num_classes, num_layers, dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,loss_fn,optimizer,train_generator, dev_generator, epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(tqdm(train_generator)):\n",
    "            model.train()\n",
    "            a, a_length=batch.premise\n",
    "            b, b_length=batch.hypothesis\n",
    "            labels=batch.label\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs=model(a,a_length,b,b_length)\n",
    "            loss=loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 10 == 0:\n",
    "                loss_history.append(loss.item())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            corr_num = 0\n",
    "            err_num = 0\n",
    "            for batch in dev_iter:\n",
    "                a, a_length=batch.premise\n",
    "                b, b_length=batch.hypothesis\n",
    "                labels=batch.label\n",
    "                outputs=model(a,a_length,b,b_length)\n",
    "                corr_num += (outputs.argmax(1) == labels).sum().item()\n",
    "                err_num += (outputs.argmax(1) != labels).sum().item()\n",
    "            tqdm.write('Epoch {}, Accuracy {}'.format(epoch, corr_num / (corr_num + err_num))) \n",
    "        torch.save(model, './model/model_'+model.name+'_epoch_{}.pkl'.format(epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7949534e67ee420e84fa215a4cb33d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy 0.7444625076204023\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/model_ESIM_epoch_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a95b407d48af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-3e72dd5a58d8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, optimizer, train_generator, dev_generator, epochs)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0merr_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {}, Accuracy {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorr_num\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcorr_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0merr_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./model/model_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_epoch_{}.pkl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/model_ESIM_epoch_0.pkl'"
     ]
    }
   ],
   "source": [
    "loss_history=[]\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train(model,loss_fn,optimizer,train_iter,dev_iter,epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
